{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517f7da4-7d3e-4676-94e8-3a2979f034c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\NLP203\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_out shape: torch.Size([2, 8, 2, 64])\n",
      "✅ RoPE looks okay if shapes match\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from rope import apply_rotary_emb\n",
    "\n",
    "q = torch.randn(2, 8, 2, 64)\n",
    "k = torch.randn(2, 8, 2, 64)\n",
    "\n",
    "q_out, k_out = apply_rotary_emb(q, k, head_dim=64, max_seq_len=8)\n",
    "print(\"q_out shape:\", q_out.shape)\n",
    "print(\"✅ RoPE looks okay if shapes match\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af6b6ac8-0aa7-43fe-98e8-b7fbb13a2c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[0.3651, 0.7303, 1.0954, 1.4606]], grad_fn=<MulBackward0>)\n",
      "✅ RMSNorm looks okay if shape is (1,4) and values are scaled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llama import RMSNorm\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0, 3.0, 4.0]])\n",
    "norm = RMSNorm(dim=4)\n",
    "output = norm(x)\n",
    "\n",
    "print(\"output:\", output)\n",
    "print(\"✅ RMSNorm looks okay if shape is (1,4) and values are scaled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d316831-e10e-439b-9b9e-7ff5eb34c969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before matmul shapes: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "attention scores (before mask): min=-2.6131701469421387, max=2.7626895904541016\n",
      "Causal mask shape: torch.Size([1, 1, 8, 8])\n",
      "attention scores (after mask): min=-inf, max=2.5095739364624023\n",
      "attention_probs min=0.0, max=1.0, sum=tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]])\n",
      "attention_output shape: torch.Size([1, 8, 8, 64]), min=-3.206193685531616, max=3.4959092140197754\n",
      "Attention output shape: torch.Size([1, 8, 8, 64])\n",
      "✅ Attention output shape OK\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llama import Attention\n",
    "from base_llama import LlamaConfig\n",
    "\n",
    "config = LlamaConfig()\n",
    "attn = Attention(config)\n",
    "attn.eval()\n",
    "\n",
    "q = torch.randn(1, config.n_heads, 8, config.dim // config.n_heads)\n",
    "k = torch.randn_like(q)\n",
    "v = torch.randn_like(q)\n",
    "\n",
    "out = attn.compute_query_key_value_scores(q, k, v)\n",
    "print(\"Attention output shape:\", out.shape)  # (1, n_heads, 8, head_dim)\n",
    "print(\"✅ Attention output shape OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b7c277-7607-4073-8df7-1670857e1b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before matmul shapes: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "attention scores (before mask): min=-1.0283336639404297, max=1.0711458921432495\n",
      "Causal mask shape: torch.Size([1, 1, 8, 8])\n",
      "attention scores (after mask): min=-inf, max=1.0278767347335815\n",
      "attention_probs min=0.0, max=1.0, sum=tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attention_output shape: torch.Size([1, 8, 8, 64]), min=-1.8616827726364136, max=1.5759408473968506\n",
      "Layer output shape: torch.Size([1, 8, 512])\n",
      "✅ LlamaLayer output OK\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llama import LlamaLayer\n",
    "from base_llama import LlamaConfig\n",
    "\n",
    "config = LlamaConfig()\n",
    "layer = LlamaLayer(0, config)\n",
    "x = torch.randn(1, 8, config.dim)\n",
    "\n",
    "out = layer(x)\n",
    "print(\"Layer output shape:\", out.shape)\n",
    "print(\"✅ LlamaLayer output OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef08ade5-c440-41ef-b0f8-0f2d9078cc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before matmul shapes: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "attention scores (before mask): min=-0.6149947047233582, max=0.5480402708053589\n",
      "Causal mask shape: torch.Size([1, 1, 8, 8])\n",
      "attention scores (after mask): min=-inf, max=0.49598225951194763\n",
      "attention_probs min=0.0, max=1.0, sum=tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attention_output shape: torch.Size([1, 8, 8, 64]), min=-1.3698432445526123, max=1.2851078510284424\n",
      "Before matmul shapes: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "attention scores (before mask): min=-0.6504539847373962, max=0.5763906240463257\n",
      "Causal mask shape: torch.Size([1, 1, 8, 8])\n",
      "attention scores (after mask): min=-inf, max=0.5748006701469421\n",
      "attention_probs min=0.0, max=1.0, sum=tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attention_output shape: torch.Size([1, 8, 8, 64]), min=-1.2589902877807617, max=1.2772756814956665\n",
      "Before matmul shapes: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "attention scores (before mask): min=-0.6668069958686829, max=0.45472949743270874\n",
      "Causal mask shape: torch.Size([1, 1, 8, 8])\n",
      "attention scores (after mask): min=-inf, max=0.45472949743270874\n",
      "attention_probs min=0.0, max=1.0, sum=tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attention_output shape: torch.Size([1, 8, 8, 64]), min=-1.3237980604171753, max=1.6021840572357178\n",
      "Before matmul shapes: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "attention scores (before mask): min=-0.5385148525238037, max=0.541309118270874\n",
      "Causal mask shape: torch.Size([1, 1, 8, 8])\n",
      "attention scores (after mask): min=-inf, max=0.541309118270874\n",
      "attention_probs min=0.0, max=1.0, sum=tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attention_output shape: torch.Size([1, 8, 8, 64]), min=-1.3170243501663208, max=1.6007733345031738\n",
      "Before matmul shapes: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "attention scores (before mask): min=-0.620251476764679, max=0.7955936193466187\n",
      "Causal mask shape: torch.Size([1, 1, 8, 8])\n",
      "attention scores (after mask): min=-inf, max=0.5670732855796814\n",
      "attention_probs min=0.0, max=1.0, sum=tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attention_output shape: torch.Size([1, 8, 8, 64]), min=-1.3418467044830322, max=1.5630598068237305\n",
      "Before matmul shapes: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "attention scores (before mask): min=-0.46607083082199097, max=0.43099719285964966\n",
      "Causal mask shape: torch.Size([1, 1, 8, 8])\n",
      "attention scores (after mask): min=-inf, max=0.3929971158504486\n",
      "attention_probs min=0.0, max=1.0, sum=tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attention_output shape: torch.Size([1, 8, 8, 64]), min=-1.7893309593200684, max=1.9240691661834717\n",
      "Before matmul shapes: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "attention scores (before mask): min=-0.5008194446563721, max=0.680306077003479\n",
      "Causal mask shape: torch.Size([1, 1, 8, 8])\n",
      "attention scores (after mask): min=-inf, max=0.5686800479888916\n",
      "attention_probs min=0.0, max=1.0, sum=tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attention_output shape: torch.Size([1, 8, 8, 64]), min=-1.3689398765563965, max=1.5518747568130493\n",
      "Before matmul shapes: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "attention scores (before mask): min=-0.5506066679954529, max=0.4817330241203308\n",
      "Causal mask shape: torch.Size([1, 1, 8, 8])\n",
      "attention scores (after mask): min=-inf, max=0.4817330241203308\n",
      "attention_probs min=0.0, max=1.0, sum=tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attention_output shape: torch.Size([1, 8, 8, 64]), min=-1.434632658958435, max=1.2991533279418945\n",
      "Logits shape: torch.Size([1, 1, 32000])\n",
      "✅ Full Llama forward pass OK\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llama import Llama\n",
    "from base_llama import LlamaConfig\n",
    "\n",
    "config = LlamaConfig()\n",
    "model = Llama(config)\n",
    "tokens = torch.randint(0, config.vocab_size, (1, 8))\n",
    "logits, hidden = model(tokens)\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"✅ Full Llama forward pass OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f7e7812-a10e-4964-81af-95c89844a6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim': 512, 'n_layers': 8, 'n_heads': 8, 'n_kv_heads': 8, 'vocab_size': 32000, 'multiple_of': 32, 'max_seq_len': 1024}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ckpt = torch.load(\"stories42M.pt\", map_location=\"cpu\")\n",
    "print(ckpt[\"model_args\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c210104-3cd4-4bb7-ad87-61ec5922efd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP203)",
   "language": "python",
   "name": "nlp203"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
